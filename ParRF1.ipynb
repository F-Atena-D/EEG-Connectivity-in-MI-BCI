{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI BCI classification using Random Forests with hyperparameters optimized in randomized and grid search \n",
    "\n",
    "Written by: Sabato Santaniello, Fatemeh Delavari [Atena]\n",
    "\n",
    "Last Edit: 04/14/2024\n",
    "\n",
    "Version: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "This script was executed on 2024-04-25 with the following libraries:\n",
      "\n",
      "-- Python ver. 3.12.2 | packaged by Anaconda, Inc. | (main, Feb 27 2024, 17:28:07) [MSC v.1916 64 bit (AMD64)]\n",
      "-- NumPy ver. 1.26.4\n",
      "-- Pandas ver. 2.2.1\n",
      "-- Scikit-Learn ver. 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd             # Pandas is for data analysis and structure manipulation\n",
    "import numpy as np              # NumPy is for numerical operations\n",
    "import sklearn                  # Scikit-Learn is for the ML algorithms\n",
    "import matplotlib               # MatPlotLib is for making plots & figures\n",
    "import matplotlib.pyplot as plt # PyPlot is a subset of the library for making MATLAB-style plots\n",
    "\n",
    "import sys                      # Optional: It is used to print out the current version of Python\n",
    "import warnings                 # Optional: It is meant to ignore warnings\n",
    "from datetime import date       # Optional: It is used to print out the date of this solution\n",
    "\n",
    "## Let us setup the font size\n",
    "plt.rcParams['axes.labelsize']  = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "print('Done!')\n",
    "\n",
    "## Print out the configuration of the solver and the current date\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(f\"This script was executed on {date.today()} with the following libraries:\\n\")\n",
    "print(f\"-- Python ver. {sys.version}\")\n",
    "print(f\"-- NumPy ver. {np.__version__}\")\n",
    "print(f\"-- Pandas ver. {pd.__version__}\")\n",
    "print(f\"-- Scikit-Learn ver. {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "path = r'C:\\Users\\Atena\\OneDrive - University of Connecticut\\DriveCbackup\\MIBCI_2024\\\\'\n",
    "plvm= loadmat(f'{path}plv1mi.mat')\n",
    "plvtdL = plvm['plvtL']\n",
    "plvtdR = plvm['plvtR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 72, 22, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plvtdL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((\n",
    "    np.zeros((72, 1)),\n",
    "    np.ones((72, 1))\n",
    "), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "## Test how the optimal classifiers perform in cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m    X \u001b[38;5;241m=\u001b[39m plvLR\n\u001b[0;32m     34\u001b[0m    X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[1;32m---> 36\u001b[0m    \u001b[43mRsearch_RF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m## Print the parameters associated with the \"best accuracy\" in cross validation\u001b[39;00m\n\u001b[0;32m     39\u001b[0m    \u001b[38;5;28mprint\u001b[39m(Rsearch_RF\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\Atena\\anaconda3\\envs\\com_detect\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Create the classifiers and search the space of hyperparameters in a pseudo-random way\n",
    "obj1 = RandomForestClassifier(random_state=81)\n",
    "\n",
    "# Cast a wide net of parameter combinations. Note that \"GridSearchCV\" will test all parameter combinations and apply cross-validation to each combination. \n",
    "# Instead, \"RandomizedSearchCV\" will test only \"n_iter\" randomly selected parameter combinations and apply cross-validation to each selected combination.\n",
    "\n",
    "param_grid = {\n",
    "   #  'criterion':['gini', 'entropy'],\n",
    "    'criterion':['gini'],\n",
    "    'n_estimators': np.arange(20, 5000, 100),\n",
    "    'min_samples_leaf': np.arange(1, 50, 5),\n",
    "    'max_features':['log2']\n",
    "   #  'max_features':['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "Y = np.concatenate((\n",
    "    np.zeros((72, 1)),\n",
    "    np.ones((72, 1))\n",
    "), axis=0)\n",
    "\n",
    "Rsearch_RF = RandomizedSearchCV(obj1, param_grid, cv=5, n_iter=100, scoring='accuracy', refit=True, return_train_score=False, n_jobs=-1,verbose=1)\n",
    "\n",
    "RFps = {}\n",
    "acc = np.zeros([9, 1])\n",
    "sis = np.zeros([9, 22*22])\n",
    "sim = np.zeros([9, 22*22])\n",
    "\n",
    "for d in range(1):\n",
    "   plvL = np.reshape(plvtdL[d, :, :, :], (72, -1))\n",
    "   plvR = np.reshape(plvtdR[d, :, :, :], (72, -1))\n",
    "   plvLR = np.concatenate((plvL, plvR), axis=0)\n",
    "   X = plvLR\n",
    "\n",
    "   X = scaler.fit_transform(X)\n",
    "\n",
    "   Rsearch_RF.fit(X, Y)\n",
    "\n",
    "## Print the parameters associated with the \"best accuracy\" in cross validation\n",
    "   print(Rsearch_RF.best_params_)\n",
    "\n",
    "   RFps[d] = [Rsearch_RF.best_params_]\n",
    "\n",
    "   rf_clf = Rsearch_RF.best_estimator_\n",
    "\n",
    "   ## Define the cross-validation. I decide to shuffle before splitting\n",
    "   split_info = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "\n",
    "## Initialize the vectors\n",
    "   Y_test = np.empty([0,1],dtype='int64')\n",
    "   Y_pred = np.empty([0,1],dtype='int64')\n",
    "   Y_pred_proba = np.empty([0,2],dtype='float64')\n",
    "\n",
    "## Cross-validation loop\n",
    "   for train_index, test_index in split_info.split(X, Y):\n",
    "    \n",
    "       ## Make the split\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, tmp = Y[train_index], Y[test_index]\n",
    "   \n",
    "      Y_test = np.vstack((Y_test, tmp.reshape(-1, 1)))\n",
    "   \n",
    "      ## Remember to apply the scaling\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      ## Fit the classifiers\n",
    "      rf_clf.fit(X_train,Y_train)\n",
    "        \n",
    "      ## Compute the preditions\n",
    "      Y_pred = np.vstack((Y_pred, rf_clf.predict(X_test).reshape(-1,1)))\n",
    "    \n",
    "      ## Compute the probabilities associated with each class\n",
    "      Y_pred_proba = np.vstack((Y_pred_proba, rf_clf.predict_proba(X_test)))\n",
    "\n",
    "   classifier = Y_pred\n",
    "   # label = 'Random Forests'\n",
    "\n",
    "   # Compute feature importance\n",
    "   feature_importance = rf_clf.feature_importances_\n",
    "   sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "   sorted_importance = feature_importance[sorted_idx]\n",
    "\n",
    "   # Store sorted indices and importance (assuming sis and sim are pre-initialized arrays)\n",
    "   # sis[d, :] = sorted_idx\n",
    "   # sim[d, :] = sorted_importance\n",
    "\n",
    "   accuracy = accuracy_score(Y_test, classifier)\n",
    "   acc[d] = accuracy\n",
    "\n",
    "   # acc = accuracy_score(Y_test, classifier)\n",
    "   print(accuracy)\n",
    "      # pre = precision_score(Y_test, classifier, average = 'weighted')\n",
    "      # rec = recall_score(Y_test, classifier, average = 'weighted')\n",
    "      # f1s = f1_score(Y_test, classifier, average = 'weighted')\n",
    "      # print(label,': Acc=',acc,'\\t Prec=',pre,'\\t Recall=',rec,'\\t F1 score=',f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the classifiers and search the space of hyperparameters in a pseudo-random way\n",
    "obj1 = RandomForestClassifier(random_state=81)\n",
    "\n",
    "# Cast a wide net of parameter combinations. Note that \"GridSearchCV\" will test all parameter combinations and apply cross-validation to each combination. \n",
    "# Instead, \"RandomizedSearchCV\" will test only \"n_iter\" randomly selected parameter combinations and apply cross-validation to each selected combination.\n",
    "\n",
    "param_grid = {\n",
    "   #  'criterion':['gini', 'entropy'],\n",
    "    'criterion':['gini'],\n",
    "    'n_estimators': np.arange(20, 5000, 100),\n",
    "    'min_samples_leaf': np.arange(1, 50, 5),\n",
    "    'max_features':['log2']\n",
    "   #  'max_features':['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "Y = np.concatenate((\n",
    "    np.zeros((72, 1)),\n",
    "    np.ones((72, 1))\n",
    "), axis=0)\n",
    "\n",
    "Rsearch_RF = RandomizedSearchCV(obj1, param_grid, cv=5, n_iter=100, scoring='accuracy', refit=True, return_train_score=False, n_jobs=-1,verbose=1)\n",
    "\n",
    "RFps = {}\n",
    "acc = np.zeros([9, 1])\n",
    "sis = np.zeros([9, 22*22])\n",
    "sim = np.zeros([9, 22*22])\n",
    "\n",
    "for d in range(1):\n",
    "   plvL = np.reshape(plvtdL[d, :, :, :], (72, -1))\n",
    "   plvR = np.reshape(plvtdR[d, :, :, :], (72, -1))\n",
    "   plvLR = np.concatenate((plvL, plvR), axis=0)\n",
    "   X = plvLR\n",
    "\n",
    "   X = scaler.fit_transform(X)\n",
    "\n",
    "   Rsearch_RF.fit(X, Y)\n",
    "\n",
    "## Print the parameters associated with the \"best accuracy\" in cross validation\n",
    "   print(Rsearch_RF.best_params_)\n",
    "\n",
    "   RFps[d] = [Rsearch_RF.best_params_]\n",
    "\n",
    "   rf_clf = Rsearch_RF.best_estimator_\n",
    "\n",
    "   ## Define the cross-validation. I decide to shuffle before splitting\n",
    "   split_info = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "\n",
    "## Initialize the vectors\n",
    "   Y_test = np.empty([0,1],dtype='int64')\n",
    "   Y_pred = np.empty([0,1],dtype='int64')\n",
    "   Y_pred_proba = np.empty([0,2],dtype='float64')\n",
    "\n",
    "## Cross-validation loop\n",
    "   for train_index, test_index in split_info.split(X, Y):\n",
    "    \n",
    "       ## Make the split\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, tmp = Y[train_index], Y[test_index]\n",
    "   \n",
    "      Y_test = np.vstack((Y_test, tmp.reshape(-1, 1)))\n",
    "   \n",
    "      ## Remember to apply the scaling\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      ## Fit the classifiers\n",
    "      rf_clf.fit(X_train,Y_train)\n",
    "        \n",
    "      ## Compute the preditions\n",
    "      Y_pred = np.vstack((Y_pred, rf_clf.predict(X_test).reshape(-1,1)))\n",
    "    \n",
    "      ## Compute the probabilities associated with each class\n",
    "      Y_pred_proba = np.vstack((Y_pred_proba, rf_clf.predict_proba(X_test)))\n",
    "\n",
    "   classifier = Y_pred\n",
    "   # label = 'Random Forests'\n",
    "\n",
    "   # Compute feature importance\n",
    "   feature_importance = rf_clf.feature_importances_\n",
    "   sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "   sorted_importance = feature_importance[sorted_idx]\n",
    "\n",
    "   # Store sorted indices and importance (assuming sis and sim are pre-initialized arrays)\n",
    "   # sis[d, :] = sorted_idx\n",
    "   # sim[d, :] = sorted_importance\n",
    "\n",
    "   accuracy = accuracy_score(Y_test, classifier)\n",
    "   acc[d] = accuracy\n",
    "\n",
    "   # acc = accuracy_score(Y_test, classifier)\n",
    "   print(accuracy)\n",
    "      # pre = precision_score(Y_test, classifier, average = 'weighted')\n",
    "      # rec = recall_score(Y_test, classifier, average = 'weighted')\n",
    "      # f1s = f1_score(Y_test, classifier, average = 'weighted')\n",
    "      # print(label,': Acc=',acc,'\\t Prec=',pre,'\\t Recall=',rec,'\\t F1 score=',f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.io as sio\n",
    "\n",
    "# lRFps = list(RFps.values())\n",
    "\n",
    "# # Create a dictionary of the variables\n",
    "# data = {\n",
    "#     'RFps': lRFps,\n",
    "#     'acc': acc,\n",
    "#     'sis': sis,\n",
    "#     'sim': sim\n",
    "# }\n",
    "\n",
    "# # Save to a .mat file\n",
    "# sio.savemat('RFp1.mat', data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "# Load the .mat file\n",
    "data = sio.loadmat('RFp1.mat')\n",
    "\n",
    "# Access the variables\n",
    "RFps = data['RFps']\n",
    "acc = data['acc']\n",
    "sis = data['sis']\n",
    "sim = data['sim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[(array([[4320]]), array([[11]]), array(['log2'], dtype='<U4'), array(['gini'], dtype='<U4'))]],\n",
       "      dtype=[('n_estimators', 'O'), ('min_samples_leaf', 'O'), ('max_features', 'O'), ('criterion', 'O')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.63888889],\n",
       "       [0.5625    ],\n",
       "       [0.75694444],\n",
       "       [0.73611111],\n",
       "       [0.48611111],\n",
       "       [0.59027778],\n",
       "       [0.58333333],\n",
       "       [0.83333333],\n",
       "       [0.6875    ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((\n",
    "    np.zeros((72, 1)),\n",
    "    np.ones((72, 1))\n",
    "), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sklearn.ensemble._forest.RandomForestClassifier() argument after ** must be a mapping, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     11\u001b[0m Bparams \u001b[38;5;241m=\u001b[39m RFps[d][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 13\u001b[0m rf_clf_P \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBparams)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     15\u001b[0m    shuffledY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(Y)\n",
      "\u001b[1;31mTypeError\u001b[0m: sklearn.ensemble._forest.RandomForestClassifier() argument after ** must be a mapping, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "acc_p = np.zeros((9, 100))\n",
    "\n",
    "for d in range(9):\n",
    "   plvL = np.reshape(plvtdL[d, :, :, :], (72, -1))\n",
    "   plvR = np.reshape(plvtdR[d, :, :, :], (72, -1))\n",
    "   plvLR = np.concatenate((plvL, plvR), axis=0)\n",
    "   X = plvLR\n",
    "\n",
    "   X = scaler.fit_transform(X)\n",
    "\n",
    "   Bparams = RFps[d][0]\n",
    "\n",
    "   rf_clf_P = RandomForestClassifier(**Bparams)\n",
    "   for rep in range(100):\n",
    "      shuffledY = np.random.permutation(Y)\n",
    "      ## Define the cross-validation. I decide to shuffle before splitting\n",
    "      split_info = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "\n",
    "   ## Initialize the vectors\n",
    "      shuffledY_test = np.empty([0,1],dtype='int64')\n",
    "      Y_pred = np.empty([0,1],dtype='int64')\n",
    "      Y_pred_proba = np.empty([0,2],dtype='float64')\n",
    "\n",
    "   ## Cross-validation loop\n",
    "      for train_index, test_index in split_info.split(X, Y):\n",
    "      \n",
    "         ## Make the split\n",
    "         X_train, X_test = X[train_index], X[test_index]\n",
    "         shuffledY_train, tmp = shuffledY[train_index], shuffledY[test_index]\n",
    "      \n",
    "         shuffledY_test = np.vstack((shuffledY_test, tmp.reshape(-1, 1)))\n",
    "      \n",
    "         ## Remember to apply the scaling\n",
    "         X_train = scaler.fit_transform(X_train)\n",
    "         X_test = scaler.transform(X_test)\n",
    "\n",
    "         ## Fit the classifiers\n",
    "         rf_clf_P.fit(X_train,shuffledY_train)\n",
    "         \n",
    "         ## Compute the preditions\n",
    "         Y_pred = np.vstack((Y_pred, rf_clf_P.predict(X_test).reshape(-1,1)))\n",
    "         \n",
    "         ## Compute the probabilities associated with each class\n",
    "         Y_pred_proba = np.vstack((Y_pred_proba, rf_clf_P.predict_proba(X_test)))\n",
    "\n",
    "      classifier = Y_pred\n",
    "      # label = 'Random Forests'\n",
    "\n",
    "      accuracy = accuracy_score(shuffledY_test, classifier)\n",
    "      acc_p[d, rep] = accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAcc1 = np.mean(acc_p, axis=0)\n",
    "find(mAcc1[d]>acc[d])\n",
    "for d in range(9):\n",
    "    length[d] = np.sum(mAcc1[d] > acc[d])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59027778 0.46527778 0.50694444 0.54861111 0.45833333 0.40277778\n",
      "  0.41666667 0.53472222 0.46527778 0.51388889 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# print(Y_pred_proba.shape)\n",
    "# print(Y_pred)\n",
    "\n",
    "# print(acc)\n",
    "# print(accuracy)\n",
    "\n",
    "print(acc_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gsearch_RF = GridSearchCV(obj1, param_grid, cv=5, scoring='accuracy', refit=True, return_train_score=False, n_jobs=-1,verbose=1)\n",
    "\n",
    "# for d in range(1):\n",
    "#    plvL = np.reshape(plvtdL[d, :, :, :], (72, -1))\n",
    "#    plvR = np.reshape(plvtdR[d, :, :, :], (72, -1))\n",
    "#    plvLR = np.concatenate((plvL, plvR), axis=0)\n",
    "#    X = plvLR\n",
    "\n",
    "#    X = scaler.fit_transform(X)\n",
    "\n",
    "#    Gsearch_RF.fit(X, Y)\n",
    "\n",
    "# ## Print the parameters associated with the \"best accuracy\" in cross validation\n",
    "#    print(Gsearch_RF.best_params_)\n",
    "\n",
    "#    rf_clf = Gsearch_RF.best_estimator_\n",
    "\n",
    "#    ## Define the cross-validation. I decide to shuffle before splitting\n",
    "#    split_info = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "\n",
    "# ## Initialize the vectors\n",
    "#    Y_test = np.empty([0,1],dtype='int64')\n",
    "#    Y_pred = np.empty([0,1],dtype='int64')\n",
    "#    Y_pred_proba = np.empty([0,2],dtype='float64')\n",
    "\n",
    "# ## Cross-validation loop\n",
    "#    for train_index, test_index in split_info.split(X, Y):\n",
    "    \n",
    "#        ## Make the split\n",
    "#       X_train, X_test = X[train_index], X[test_index]\n",
    "#       Y_train, tmp = Y[train_index], Y[test_index]\n",
    "   \n",
    "#       Y_test = np.vstack((Y_test, tmp.reshape(-1, 1)))\n",
    "   \n",
    "#       ## Remember to apply the scaling\n",
    "#       X_train = scaler.fit_transform(X_train)\n",
    "#       X_test = scaler.transform(X_test)\n",
    "\n",
    "#       ## Fit the classifiers\n",
    "#       rf_clf.fit(X_train,Y_train)\n",
    "        \n",
    "#       ## Compute the preditions\n",
    "#       Y_pred = np.vstack((Y_pred, rf_clf.predict(X_test).reshape(-1,1)))\n",
    "    \n",
    "#       ## Compute the probabilities associated with each class\n",
    "#       Y_pred_proba = np.vstack((Y_pred_proba, rf_clf.predict_proba(X_test)))\n",
    "\n",
    "#    classifier = Y_pred\n",
    "#    # label = 'Random Forests'\n",
    "\n",
    "#    acc = accuracy_score(Y_test, classifier)\n",
    "#    print(acc)\n",
    "#       # pre = precision_score(Y_test, classifier, average = 'weighted')\n",
    "#       # rec = recall_score(Y_test, classifier, average = 'weighted')\n",
    "#       # f1s = f1_score(Y_test, classifier, average = 'weighted')\n",
    "#       # print(label,': Acc=',acc,'\\t Prec=',pre,'\\t Recall=',rec,'\\t F1 score=',f1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sklearn.ensemble._forest.RandomForestClassifier() argument after ** must be a mapping, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     13\u001b[0m Bparams \u001b[38;5;241m=\u001b[39m RFps[d][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 15\u001b[0m rf_clf_P \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mBparams)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m## Define the cross-validation. I decide to shuffle before splitting\u001b[39;00m\n\u001b[0;32m     18\u001b[0m split_info \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m95\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sklearn.ensemble._forest.RandomForestClassifier() argument after ** must be a mapping, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "acc1p = np.zeros((9, 1))\n",
    "\n",
    "for d in range(9):\n",
    "   plvL = np.reshape(plvtdL[d, :, :, :], (72, -1))\n",
    "   plvR = np.reshape(plvtdR[d, :, :, :], (72, -1))\n",
    "   plvLR = np.concatenate((plvL, plvR), axis=0)\n",
    "   XX = plvLR\n",
    "\n",
    "   X = XX[:, 1:5]\n",
    "\n",
    "   X = scaler.fit_transform(X)\n",
    "\n",
    "   Bparams = RFps[d][0]\n",
    "\n",
    "   rf_clf_P = RandomForestClassifier(**Bparams)\n",
    "\n",
    "   ## Define the cross-validation. I decide to shuffle before splitting\n",
    "   split_info = StratifiedKFold(n_splits=5, shuffle=True, random_state=95)\n",
    "\n",
    "## Initialize the vectors\n",
    "   Y_test = np.empty([0,1],dtype='int64')\n",
    "   Y_pred = np.empty([0,1],dtype='int64')\n",
    "   Y_pred_proba = np.empty([0,2],dtype='float64')\n",
    "\n",
    "## Cross-validation loop\n",
    "   for train_index, test_index in split_info.split(X, Y):\n",
    "    \n",
    "       ## Make the split\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      Y_train, tmp = Y[train_index], Y[test_index]\n",
    "   \n",
    "      Y_test = np.vstack((Y_test, tmp.reshape(-1, 1)))\n",
    "   \n",
    "      ## Remember to apply the scaling\n",
    "      X_train = scaler.fit_transform(X_train)\n",
    "      X_test = scaler.transform(X_test)\n",
    "\n",
    "      ## Fit the classifiers\n",
    "      rf_clf.fit(X_train,Y_train)\n",
    "        \n",
    "      ## Compute the preditions\n",
    "      Y_pred = np.vstack((Y_pred, rf_clf.predict(X_test).reshape(-1,1)))\n",
    "    \n",
    "      ## Compute the probabilities associated with each class\n",
    "      Y_pred_proba = np.vstack((Y_pred_proba, rf_clf.predict_proba(X_test)))\n",
    "\n",
    "   classifier = Y_pred\n",
    "   # label = 'Random Forests'\n",
    "\n",
    "   # Compute feature importance\n",
    "   feature_importance = rf_clf.feature_importances_\n",
    "\n",
    "   accuracy = accuracy_score(Y_test, classifier)\n",
    "   acc1p[d] = accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "com_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
